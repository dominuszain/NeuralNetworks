{"cells":[{"cell_type":"markdown","metadata":{},"source":["Tensorflow is a really powerful framework for machine learning. For my purposes of simple feed-forward neural networks, Keras would be enough."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models"]},{"cell_type":"markdown","metadata":{},"source":["Later, I will make guides on how to use custom datasets from local memory, but for now, the following is how you download and use publically available datasets. We are normalizing the greyscale images, so that, instead of ranging from 0 to 225, they now range from 0 to 1, which is really convenient."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["(train_images, train_label), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","train_images = train_images.astype('float32') / 225\n","test_images = test_images.astype('float32') / 225"]},{"cell_type":"markdown","metadata":{},"source":["import matplotlib.pyplot as plt\n","\n","n = 958\n","plt.imshow(train_images[n], cmap='gray')\n","plt.title(f\"Label: {train_label[n]}\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["train_images and test_images objects are tensors. I can create my own tensors using the tf.Variable() and tf.zeros() methods. train_images.shape() is going to tell us the dimensions of this tensor."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["model = models.Sequential([\n","    layers.Input(shape=(28,28,1)),\n","    layers.Flatten(),\n","    layers.Dense(128,activation='relu'),\n","    layers.Dense(64,activation='relu'),\n","    layers.Dense(10,activation='softmax')\n","])"]},{"cell_type":"markdown","metadata":{},"source":["The above piece of code is really important. It is vital that everything there must be understood. The 'Sequential' method is used for feed-forward ordinary neural networks, which I will be using almost all of the times. The first layer is the input layer of our neural network. The 'Flatten' method is used to convert the input matrix to a vector i.e. flatten it out. In our case, the second and third layers are what are called 'hidden layers'. These hidden layers can have as many neurons as possible. The activation function is what normalizes the output of neurons to a specific format. The 'relu' activation function stands for rectified linear unit, which is good enough for most of our applications. The 'Dense' method refers to the fact that the neurons are densly connected to each other i.e. each neuron is connected to every other neuron after it. The last layer is always the output layer, containing as many neurons as our outputs. The 'softmax' activation function in the output is used when we want a probablistic output, normalized to unity. "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["Optimizer refers to the backpropogation algorithm that adjusts the weights. 'adam' is pretty good and should be left as is. Similarly, loss function refers to the method used for quantizing how much closer we are to the desired results. As previously, the current one should be left as is, unless you have a good reason."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8751 - loss: 0.4166\n","Epoch 2/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.1015\n","Epoch 3/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.0644\n","Epoch 4/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0506\n","Epoch 5/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0394\n","Epoch 6/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0304\n","Epoch 7/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9918 - loss: 0.0249\n","Epoch 8/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0214\n","Epoch 9/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0209\n","Epoch 10/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0160\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7f4cb2b10e50>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(train_images,train_label,epochs=10)"]},{"cell_type":"markdown","metadata":{},"source":["Here we train the model by specifying the input data (train_images) and the actual data the input is supposed to correspond to (train_label). Epochs are how many times the training data is fed to the model. They should not be too high to avoid overfitting. A good balance should be determined based on the training accuracy and the evaluated accuracy."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9709 - loss: 0.1214\n"]}],"source":["test_loss, test_accuracy = model.evaluate(test_images, test_labels)"]},{"cell_type":"markdown","metadata":{},"source":["That's it! If your fundamentals are good, training and using simple neural networks using Tensorflow isn't that hard. Now you can modify the structure of this code as per your needs. Best of Luck!!"]}],"metadata":{"kernelspec":{"display_name":"tensorflow","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":2}
