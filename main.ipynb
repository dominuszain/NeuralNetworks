{"cells":[{"cell_type":"markdown","metadata":{},"source":["Tensorflow is a really powerful framework for machine learning. For my purposes of simple feed-forward neural networks, Keras would be enough."]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models"]},{"cell_type":"markdown","metadata":{},"source":["Later, I will make guides on how to use custom datasets from local memory, but for now, the following is how you download and use publically available datasets. We are normalizing the greyscale images, so that, instead of ranging from 0 to 225, they now range from 0 to 1, which is really convenient."]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":["(train_images, train_label), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","train_images = train_images.astype('float32') / 225\n","test_images = test_images.astype('float32') / 225"]},{"cell_type":"markdown","metadata":{},"source":["#This is how you know that train_images tensor contains the 28x28 greyscale images and the train_label vector contains which number those images correspond to.\n","\n","import matplotlib.pyplot as plt\n","\n","n = 958\n","plt.imshow(train_images[n], cmap='gray')\n","plt.title(f\"Label: {train_label[n]}\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["train_images and test_images objects are tensors. I can create my own tensors using the tf.Variable() and tf.zeros() methods. train_images.shape() is going to tell us the dimensions of this tensor."]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zain/tensorflow/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]}],"source":["model = models.Sequential([\n","    layers.Flatten(input_shape=(28,28,1)),\n","    layers.Dense(128,activation='relu'),\n","    layers.Dense(64,activation='relu'),\n","    layers.Dense(10,activation='softmax')\n","])\n","\n","# This is what you would do if your input is already a vector.\n","#model = models.Sequential([\n","#    layers.Dense(128, activation='relu', input_shape=(10,)),  # input is already a flat 1D vector of length 10\n","#    layers.Dense(64, activation='relu'),\n","#    layers.Dense(10, activation='softmax')  # assuming 10 output classes\n","#])"]},{"cell_type":"markdown","metadata":{},"source":["The above piece of code is really important. It is vital that everything there must be understood. The 'Sequential' method is used for feed-forward ordinary neural networks, which I will be using almost all of the times. The first layer is the input layer of our neural network. The 'Flatten' method is used to convert the input matrix to a vector i.e. flatten it out. In our case, the second and third layers are what are called 'hidden layers'. These hidden layers can have as many neurons as possible. The activation function is what normalizes the output of neurons to a specific format. The 'relu' activation function stands for rectified linear unit, which is good enough for most of our applications. The 'Dense' method refers to the fact that the neurons are densly connected to each other i.e. each neuron is connected to every other neuron after it. The last layer is always the output layer, containing as many neurons as our outputs. The 'softmax' activation function in the output is used when we want a probablistic output, normalized to unity. "]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["Optimizer refers to the backpropogation algorithm that adjusts the weights. 'adam' is pretty good and should be left as is. Similarly, loss function refers to the method used for quantizing how much closer we are to the desired results. As previously, the current one should be left as is, unless you have a good reason."]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8749 - loss: 0.4218\n","Epoch 2/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1095\n","Epoch 3/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0717\n","Epoch 4/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0535\n","Epoch 5/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0406\n","Epoch 6/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.0321\n","Epoch 7/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0247\n","Epoch 8/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0204\n","Epoch 9/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0243\n","Epoch 10/10\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0166\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7fdcd91f2750>"]},"execution_count":153,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(train_images,train_label,epochs=10)"]},{"cell_type":"markdown","metadata":{},"source":["Here we train the model by specifying the input data (train_images) and the actual data the input is supposed to correspond to (train_label). Epochs are how many times the training data is fed to the model. They should not be too high to avoid overfitting. A good balance should be determined based on the training accuracy and the evaluated accuracy."]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9727 - loss: 0.1076\n"]}],"source":["test_loss, test_accuracy = model.evaluate(test_images, test_labels)"]},{"cell_type":"markdown","metadata":{},"source":["That's it! If your fundamentals are good, training and using simple neural networks using Tensorflow isn't that hard. Now you can modify the structure of this code as per your needs. Best of Luck!!"]}],"metadata":{"kernelspec":{"display_name":"tensorflow","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":2}
